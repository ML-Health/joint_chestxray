{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deal with writing a csv file for all the reports - no distinction between train and test data; only for the supervised case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Geeticka Chauhan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using this, need to make sure that data conversion for the language modeling has been run to create the csv file from all reports and process them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "import pandas as pd\n",
    "import joint_img_txt.data.text.utils as utils\n",
    "import joint_img_txt.data.text.lm_utils as lm_utils\n",
    "import re\n",
    "import joint_img_txt.data.text.preprocess as preprocess\n",
    "\n",
    "# keep below as the directory where three types of files are located: 1) class{0,1,2,3}.txt files 2) split.csv files\n",
    "class_info_dir = '/data/vision/polina/projects/chestxray/work_space_v2/report_processing/edema_labels-12-03-2019/'\n",
    "# keep below as the directory where gold standard labels are present; likely your processing \n",
    "# will be different from mine because my csv format was different from what you will have\n",
    "seth_info_dir = '/data/vision/polina/projects/chestxray/geeticka/class_information/' # class information and reports are located here\n",
    "# below is the location of the original MIMIC-CXR txt files\n",
    "raw_reports_dir = '/data/vision/polina/projects/chestxray/data_v2/reports/'\n",
    "# below is where you would like to keep the .tsv files\n",
    "out_dir = '/data/vision/polina/projects/chestxray/geeticka/pre-processed/supervised/'\n",
    "# below is the latest .csv split file; this should have the most up to date version of the labels as the model \n",
    "# will end up using the labels present in this file\n",
    "latest_csv = '/data/vision/polina/projects/chestxray/work_space_v2/report_processing/edema_labels-12-03-2019/mimic-cxr-sub-img-edema-split.csv'\n",
    "# this is where the outputted csv files will be located inside the directory reports_list/\n",
    "\n",
    "def class_info_res(filename):\n",
    "    return os.path.join(class_info_dir, filename)\n",
    "def seth_labels_res(filename):\n",
    "    return os.path.join(seth_info_dir, filename)\n",
    "def raw_reports_res(filename):\n",
    "    return os.path.join(raw_reports_dir, filename)\n",
    "def out_res(filename): return os.path.join(out_dir, filename)\n",
    "def lm_res(filename): return os.path.join('/data/vision/polina/projects/chestxray/geeticka/pre-processed/', filename)\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following are the different classes with their keywords:\n",
    "class0 None: 'no pulmonary edema', 'no vascular congestion', 'no fluid overload’, 'no acute cardiopulmonary    process’                                                                                                           \n",
    "\n",
    "class1 Vascular congestion: 'mild pulmonary vascular congestion', ‘cephalization', 'mild hilar engorgement','mild vascular plethora'                                                                                                \n",
    "\n",
    "class2 Interstitial edema: 'kerley', 'interstitial edema’, 'interstitial thickening’, 'interstitial opacities’\n",
    "\n",
    "class3 Alveolar edema: 'perihilar infiltrates', 'peri-hilar infiltrates', 'hilar infiltrates', 'alveolar infiltrates', 'severe pulmonary edema’ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edema_pred_dict = {'none': 0 ,'vascular congestion': 1, 'interstitial edema': 2, 'alveolar edema': 3}\n",
    "# might be interesting to consider the ranking loss function for the classification task in the future\n",
    "# this needs to be used when generating the test data csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(latest_csv)\n",
    "def convert_to_filename(row):\n",
    "    filename = row['study_id']\n",
    "    study_id = 's'+ str(filename) + '.txt'\n",
    "    return study_id\n",
    "data['study_id'] = data.apply(convert_to_filename, axis=1)\n",
    "data_edema = data.loc[data['edeme_severity'] >= 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_edema = data_edema.drop_duplicates(subset=['study_id', 'edeme_severity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6660"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_edema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data_edema` is less because lateral views of some images is not available, so we lost some reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train.csv contains duplicates from test.csv - need to remove those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df_filename = utils.train_filename_df(class_info_res) # this data is incomplete in terms of the keywords that \n",
    "# I have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6710"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_edema.rename(columns={'study_id':'filename',\n",
    "#                           'Edema severity':'edema_severity'}, \n",
    "#                  inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_edema = data_edema[['filename', 'edema_severity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_edema['metadata'] = '{}'\n",
    "# train_df_filename = data_edema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next todo: Need to write the test file and then double check if all the reports in the folder are located. Also check if there is any report overlap for train vs test. Also while writing the test file, need to make sure that s1232.txt appears rather than 1232.txt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the train and test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Radiologist (Seth) did not label report: s52375169.txt\n",
      "Original test data frame is 199 long but there are 21 duplicated rows\n",
      "Removed duplicates to return 178 length dataframe\n"
     ]
    }
   ],
   "source": [
    "test_df_filename, duplicated_filenames = utils.test_filename_df(seth_labels_res, edema_pred_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_filenames = test_df_filename['filename'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178 filenames from the test data are present in the train_filename.csv file. These will need to be removed\n"
     ]
    }
   ],
   "source": [
    "train_filenames_overlap = train_df_filename[train_df_filename['filename'].isin(test_filenames)]['filename'].unique().tolist()\n",
    "print(\"%d filenames from the test data are present in the train_filename.csv file. These will need to be removed\"%(\n",
    "len(train_filenames_overlap)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(test_filenames) - set(train_filenames_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above set should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing test filenames from the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df_filename = train_df_filename[~train_df_filename['filename'].isin(test_filenames)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_filename = pd.concat([train_df_filename, test_df_filename], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, `all_data` contains the labels of the train files from the keyword matching, but the test files from seth labels. This is necessary for the testing stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the train and test filename csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not(os.path.exists(out_res('reports_list'))):\n",
    "    os.mkdir(out_res('reports_list'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utils.write_dataframe(all_data_filename, out_res('reports_list/all_data_list.csv'))\n",
    "# # only write the above dataframe when the duplicates from the test data have been removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's grab the actual text located in these and tokenize it. We will use scispacy to do the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_filenames = all_data_filename['filename'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6710"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # take in a line in a report and do some pre-processing like number normalization, sentence segmentation etc\n",
    "# def pre_process_report(line_report):\n",
    "#     # do this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_reports_df = lm_utils.read_dataframe(lm_res('lm_reports/original_reports.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_reports = utils.write_report_into_df(all_data_filename, original_reports_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the train and test files with original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(out_res('reports')):\n",
    "    os.mkdir(out_res('reports'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utils.write_dataframe(all_data_reports, out_res('reports/all_data.csv'))\n",
    "# # # only write the above dataframe when the duplicates from the test data have been removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# indexes = [2,10,18,34,40,48,55,61,63,70,72,73,74,76,88,101,106,119,147,148,155,159,165,169,170]\n",
    "# for metadata in test_df_reports.iloc[indexes]['metadata']:\n",
    "#     print(metadata['keywords_found'])\n",
    "# #     print(report)\n",
    "# #     print(report['metadata']['keywords_found'], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check if any of the filenames in the original csv that ray is using is not present in all_data_reports - that should not be the case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import scispacy\n",
    "nlp = spacy.load('en_core_sci_md')\n",
    "# from scispacy.umls_linking import UmlsEntityLinker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# linker = UmlsEntityLinker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nlp.add_pipe(linker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# punctuations = ['.', ',', ':', '?', '!', ';', '-', '(', ')', '{', '}', '\"', \"'\"]\n",
    "# def is_punct(char):\n",
    "#     if char in punctuations:\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_reports['normalized_report'] = all_data_reports.apply(preprocess.normalize_report, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the reports with normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(out_res('reports_normalized')):\n",
    "    os.mkdir(out_res('reports_normalized'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utils.write_dataframe(all_data_reports, out_res('reports_normalized/all_data_original.csv'))\n",
    "# # only write the above dataframe when the duplicates from the test data have been removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utils.write_dataframe(test_df_reports, out_res('reports_normalized/test_original.csv'))\n",
    "# # only write the above dataframe when the duplicates from the test data have been removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOW, the next step is to use the script located in chestxray_joint/data/text/data_splitting.py to generate the tsv files\n",
    "\n",
    "This is the data conversion step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_channel_encoding = 'multiclass' # or multilabel\n",
    "training_mode = 'supervised' # or semisupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: fill this in order to do the generation of the tsv files as well as make the code more modular\n",
    "tsv_in_dir = '/data/vision/polina/projects/chestxray/geeticka/pre-processed/' + training_mode + '/reports_normalized'\n",
    "tsv_out_dir = '/data/vision/polina/projects/chestxray/geeticka/bert/converted_data/' + output_channel_encoding + \\\n",
    "'/' + training_mode\n",
    "\n",
    "def tsv_in_res(filename): return os.path.join(tsv_in_dir, filename)\n",
    "def tsv_out_res(filename): return os.path.join(tsv_out_dir, filename)\n",
    "\n",
    "# development_or_test = 'development'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_df = utils.read_dataframe(tsv_in_res('all_data_original.csv'))\n",
    "# test_df = utils.read_dataframe(tsv_in_res('test_original.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(tsv_out_res('full')):\n",
    "    os.makedirs(tsv_out_res('full'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the newly formed train and dev files (that are taken from original train data and are only to be used for tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_all_data_df_bert = utils.get_df_bert_multilabel(all_data_df, output_channel_encoding)\n",
    "new_all_data_df_bert.to_csv(tsv_out_res('full/all_data.tsv'), sep='\\t', index=False, header=False)\n",
    "# # don't need to keep rewriting these so can comment above out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new_dev_df_bert = utils.get_df_bert_multilabel(new_dev_df)\n",
    "# new_dev_df_bert.to_csv(tsv_out_res('development/dev.tsv'), sep='\\t', index=False, header=False)\n",
    "# # don't need to keep rewriting these so can comment above out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check if Ray's splits csv file contains any filenames I don't have in the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# directory = '/data/vision/polina/projects/chestxray/work_space_v2/report_processing/edema_labels-7-11-2019/mimic-cxr-sub-img-edema-finding-split_arranged.csv'\n",
    "# data = pd.read_csv(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def convert_to_filename(row):\n",
    "#     filename = row['study_id']\n",
    "#     study_id = 's'+ str(filename) + '.txt'\n",
    "#     return study_id\n",
    "# data['study_id'] = data.apply(convert_to_filename, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_filenames_by_img_model = data['study_id'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_filenames_by_txt_model = all_data_df['filename'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15837"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(all_filenames_by_img_model) # these are all the reports; they don't all have edema severity labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3045"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(all_filenames_by_txt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_edema = data.loc[data['Edema severity'] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# edema_all_filenames_by_img_model = data_edema['study_id'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3022"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(edema_all_filenames_by_img_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(set(edema_all_filenames_by_img_model) - set(all_filenames_by_txt_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the original train and test files (that are to be used for reporting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_df_bert = utils.get_df_bert_multilabel(train_df)\n",
    "# train_df_bert.to_csv(tsv_out_res('testing/train.tsv'), sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_df_bert = utils.get_df_bert_multilabel(test_df)\n",
    "# test_df_bert.to_csv(tsv_out_res('testing/dev.tsv'), sep='\\t', index=False, header=False)\n",
    "# # we must call it dev for the purposes of the evaluation - that is just the name that the algorithm expects\n",
    "# # this can probably be changed in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {1:'sd', 2:'sdas'}\n",
    "list(a.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deal with writing a csv file for all the reports - no distinction between train and test data; only for the unsupervised case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Geeticka Chauhan\n",
    "\n",
    "Please refer to the supervised version of this notebook for documentation related to similar components in both notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using this, need to make sure that data conversion for the language modeling has been run to create the csv file from all reports and process them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "import pandas as pd\n",
    "import chestxray_joint.data.text.utils as utils\n",
    "import chestxray_joint.data.text.lm_utils as lm_utils\n",
    "import re\n",
    "import chestxray_joint.data.text.preprocess as preprocess\n",
    "\n",
    "allCHF_or_allCXR = \"allCXR\"\n",
    "\n",
    "class_info_dir = '/data/vision/polina/projects/chestxray/work_space_v2/report_processing/edema_labels-12-03-2019/'\n",
    "seth_info_dir = '/data/vision/polina/projects/chestxray/geeticka/class_information/' # class information and reports are located here\n",
    "raw_reports_dir = '/data/vision/polina/projects/chestxray/data_v2/reports/'\n",
    "out_dir = '/data/vision/polina/projects/chestxray/geeticka/pre-processed/semisupervised/' + allCHF_or_allCXR + '/'\n",
    "# latest_csv = '/data/vision/polina/projects/chestxray/work_space_v2/report_processing/edema_labels-12-03-2019/mimic-cxr-sub-img-edema-split.csv'\n",
    "latest_csv = '/data/vision/polina/projects/chestxray/work_space_v2/report_processing/edema_labels-12-03-2019/mimic-cxr-sub-img-edema-split-{}.csv'.format(allCHF_or_allCXR)\n",
    "# this is where the outputted csv files will be located inside the directory reports_list/\n",
    "\n",
    "def class_info_res(filename):\n",
    "    return os.path.join(class_info_dir, filename)\n",
    "def seth_labels_res(filename):\n",
    "    return os.path.join(seth_info_dir, filename)\n",
    "def raw_reports_res(filename):\n",
    "    return os.path.join(raw_reports_dir, filename)\n",
    "def out_res(filename): return os.path.join(out_dir, filename)\n",
    "def lm_res(filename): return os.path.join('/data/vision/polina/projects/chestxray/geeticka/pre-processed/', filename)\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following are the different classes with their keywords:\n",
    "class0 None: 'no pulmonary edema', 'no vascular congestion', 'no fluid overload’, 'no acute cardiopulmonary    process’                                                                                                           \n",
    "\n",
    "class1 Vascular congestion: 'mild pulmonary vascular congestion', ‘cephalization', 'mild hilar engorgement','mild vascular plethora'                                                                                                \n",
    "\n",
    "class2 Interstitial edema: 'kerley', 'interstitial edema’, 'interstitial thickening’, 'interstitial opacities’\n",
    "\n",
    "class3 Alveolar edema: 'perihilar infiltrates', 'peri-hilar infiltrates', 'hilar infiltrates', 'alveolar infiltrates', 'severe pulmonary edema’ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edema_pred_dict = {'none': 0 ,'vascular congestion': 1, 'interstitial edema': 2, 'alveolar edema': 3}\n",
    "# might be interesting to consider the ranking loss function for the classification task in the future\n",
    "# this needs to be used when generating the test data csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/vision/polina/shared_software/anaconda3-4.3.1/envs/chestxray_joint/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(latest_csv)\n",
    "def convert_to_filename(row):\n",
    "    filename = row['study_id']\n",
    "    study_id = 's'+ str(filename) + '.txt'\n",
    "    return study_id\n",
    "data['study_id'] = data.apply(convert_to_filename, axis=1)\n",
    "# data_edema = data.loc[data['edeme_severity'] >= 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(subset=['study_id', 'edeme_severity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222254"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data) # this is just to get the reports, that's why the number is less than 17,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data` is less because lateral views of some images is not available, so we lost some reports. For all CXR we have 224673 reports but above is less by 2000 reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train.csv contains duplicates from test.csv - need to remove those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this represents the supervised dataset that we have access to. The rest of the reports are unlabeled\n",
    "train_df_filename = utils.train_filename_df(class_info_res) # this data is incomplete in terms of the keywords that \n",
    "# I have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6710"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the supervised part of the data, so we could find keywords there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_edema.rename(columns={'study_id':'filename',\n",
    "#                           'Edema severity':'edema_severity'}, \n",
    "#                  inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_edema = data_edema[['filename', 'edema_severity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_edema['metadata'] = '{}'\n",
    "# train_df_filename = data_edema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next todo: Need to write the test file and then double check if all the reports in the folder are located. Also check if there is any report overlap for train vs test. Also while writing the test file, need to make sure that s1232.txt appears rather than 1232.txt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the train and test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Radiologist (Seth) did not label report: s52375169.txt\n",
      "Original test data frame is 199 long but there are 21 duplicated rows\n",
      "Removed duplicates to return 178 length dataframe\n"
     ]
    }
   ],
   "source": [
    "test_df_filename, duplicated_filenames = utils.test_filename_df(seth_labels_res, edema_pred_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_filenames = test_df_filename['filename'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178 filenames from the test data are present in the train_filename.csv file. These will need to be removed\n"
     ]
    }
   ],
   "source": [
    "train_filenames_overlap = train_df_filename[train_df_filename['filename'].isin(test_filenames)]['filename'].unique().tolist()\n",
    "print(\"%d filenames from the test data are present in the train_filename.csv file. These will need to be removed\"%(\n",
    "len(train_filenames_overlap)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(test_filenames) - set(train_filenames_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above set should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing test filenames from the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df_filename = train_df_filename[~train_df_filename['filename'].isin(test_filenames)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_filename = pd.concat([train_df_filename, test_df_filename], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "supervised_filenames = list(all_data_filename['filename'])\n",
    "data_unsupervised_edema = data[['study_id', 'edeme_severity']]\n",
    "data_unsupervised_edema = data_unsupervised_edema[~data_unsupervised_edema['study_id'].isin(supervised_filenames)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222304"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_unsupervised_edema) + len(supervised_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_frontal_view_filenames = list(set(train_df_filename['filename']) - set(data['study_id']))\n",
    "# looks like the CHF data split is missing 50 files that are present in the class files. \n",
    "# splits file needs to be fixed to incorporate these in\n",
    "# above don't have any frontal view. Remove those. \n",
    "all_data_filename = all_data_filename[~all_data_filename['filename'].isin(no_frontal_view_filenames)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222254"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data_filename) + len(data_unsupervised_edema)\n",
    "# now this matches the number of files that we have in the splits file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unsupervised_data_filename = data_unsupervised_edema.copy()\n",
    "unsupervised_data_filename.rename(columns={'study_id':'filename', 'edeme_severity': 'edema_severity'}, inplace=True)\n",
    "def return_blank_keywords(row):\n",
    "    return {'keywords_found': []}\n",
    "unsupervised_data_filename['metadata'] = unsupervised_data_filename.apply(return_blank_keywords, axis=1)\n",
    "# unsupervised_data_filename['metadata'] = {'keywords_found': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_filename = pd.concat([all_data_filename, unsupervised_data_filename], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, `all_data` contains the labels of the train files from the keyword matching, but the test files from seth labels. This is necessary for the testing stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the train and test filename csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not(os.path.exists(out_res('reports_list'))):\n",
    "    os.mkdir(out_res('reports_list'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utils.write_dataframe(all_data_filename, out_res('reports_list/all_data_list.csv'))\n",
    "# # only write the above dataframe when the duplicates from the test data have been removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's grab the actual text located in these and tokenize it. We will use scispacy to do the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_filenames = all_data_filename['filename'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222254"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # take in a line in a report and do some pre-processing like number normalization, sentence segmentation etc\n",
    "# def pre_process_report(line_report):\n",
    "#     # do this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_reports_df = lm_utils.read_dataframe(lm_res('lm_reports/original_reports.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430d41b705d14c1fabc11da40c48fa44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=222254), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report s54168089.txt is empty\n"
     ]
    }
   ],
   "source": [
    "all_data_reports, files_without_sections = utils.write_report_into_df(all_data_filename, original_reports_df, semisupervised=True)\n",
    "# this will append all content in final_report when nothing can be found for the findings and impressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10716"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_without_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "percentage = float(files_without_sections)/len(all_data_reports)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.82 percent of the reports did not have finding, impression, conclusion and recommendation sections\n"
     ]
    }
   ],
   "source": [
    "print(\"%.2f percent of the reports did not have finding, impression, conclusion and recommendation sections\" %(percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222254"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data_reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the train and test files with original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(out_res('reports')):\n",
    "    os.mkdir(out_res('reports'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utils.write_dataframe(all_data_reports, out_res('reports/all_data.csv'))\n",
    "# # # only write the above dataframe when the duplicates from the test data have been removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# indexes = [2,10,18,34,40,48,55,61,63,70,72,73,74,76,88,101,106,119,147,148,155,159,165,169,170]\n",
    "# for metadata in test_df_reports.iloc[indexes]['metadata']:\n",
    "#     print(metadata['keywords_found'])\n",
    "# #     print(report)\n",
    "# #     print(report['metadata']['keywords_found'], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check if any of the filenames in the original csv that ray is using is not present in all_data_reports - that should not be the case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import scispacy\n",
    "nlp = spacy.load('en_core_sci_md')\n",
    "# from scispacy.umls_linking import UmlsEntityLinker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# linker = UmlsEntityLinker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nlp.add_pipe(linker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# punctuations = ['.', ',', ':', '?', '!', ';', '-', '(', ')', '{', '}', '\"', \"'\"]\n",
    "# def is_punct(char):\n",
    "#     if char in punctuations:\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_reports['normalized_report'] = all_data_reports.apply(preprocess.normalize_report, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the reports with normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(out_res('reports_normalized')):\n",
    "    os.mkdir(out_res('reports_normalized'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utils.write_dataframe(all_data_reports, out_res('reports_normalized/all_data_original.csv'))\n",
    "# # only write the above dataframe when the duplicates from the test data have been removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utils.write_dataframe(test_df_reports, out_res('reports_normalized/test_original.csv'))\n",
    "# # only write the above dataframe when the duplicates from the test data have been removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOW, the next step is to use the script located in chestxray_joint/data/text/data_splitting.py to generate the tsv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_channel_encoding = 'multiclass' # or multilabel\n",
    "training_mode = 'semisupervised' # or semisupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: fill this in order to do the generation of the tsv files as well as make the code more modular\n",
    "tsv_in_dir = '/data/vision/polina/projects/chestxray/geeticka/pre-processed/' + training_mode \\\n",
    "+ '/' + allCHF_or_allCXR + '/reports_normalized'\n",
    "tsv_out_dir = '/data/vision/polina/projects/chestxray/geeticka/bert/converted_data/' + output_channel_encoding + \\\n",
    "'/' + training_mode\n",
    "\n",
    "if training_mode == 'semisupervised':\n",
    "    tsv_out_dir += '/' + allCHF_or_allCXR\n",
    "\n",
    "def tsv_in_res(filename): return os.path.join(tsv_in_dir, filename)\n",
    "def tsv_out_res(filename): return os.path.join(tsv_out_dir, filename)\n",
    "\n",
    "# development_or_test = 'development'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_df = utils.read_dataframe(tsv_in_res('all_data_original.csv'))\n",
    "# test_df = utils.read_dataframe(tsv_in_res('test_original.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_data_df.fillna('', inplace=True) # there were some NaN values in original_report and normalized_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(tsv_out_res('full')):\n",
    "    os.makedirs(tsv_out_res('full'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the newly formed train and dev files (that are taken from original train data and are only to be used for tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to figure out what to do about these empty reports now that we have the semi supervised case. The report extraction needs to be made more sophisticated, based on the label. If there is no label, then maybe extract the whole final_report section? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222254"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>edema_severity</th>\n",
       "      <th>original_report</th>\n",
       "      <th>metadata</th>\n",
       "      <th>normalized_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>184233</th>\n",
       "      <td>s56406919.txt</td>\n",
       "      <td>-1</td>\n",
       "      <td>DEID -year-old male with aortic stenosis, cong...</td>\n",
       "      <td>{'keywords_found': [], 'origin_section': ['fin...</td>\n",
       "      <td>{'sentences': ['-year-old male with aortic ste...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             filename  edema_severity  \\\n",
       "184233  s56406919.txt              -1   \n",
       "\n",
       "                                          original_report  \\\n",
       "184233  DEID -year-old male with aortic stenosis, cong...   \n",
       "\n",
       "                                                 metadata  \\\n",
       "184233  {'keywords_found': [], 'origin_section': ['fin...   \n",
       "\n",
       "                                        normalized_report  \n",
       "184233  {'sentences': ['-year-old male with aortic ste...  "
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_df.loc[all_data_df['filename'] == 's56406919.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s54168089.txt is empty\n"
     ]
    }
   ],
   "source": [
    "for idx, row in all_data_df.iterrows():\n",
    "#     print(row['filename'])\n",
    "    if not utils.remove_whitespace(row['original_report']):\n",
    "        print('%s is empty'%row['filename'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like no report is empty anymore! In the future, can insert another flag for empty reports when reading the tsv file from the code in `chestxray_joint.model` and skip over the reports and images associated with those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_all_data_df_bert = utils.get_df_bert(all_data_df, output_channel_encoding)\n",
    "# new_all_data_df_bert.to_csv(tsv_out_res('full/all_data.tsv'), sep='\\t', index=False, header=False)\n",
    "# # don't need to keep rewriting these so can comment above out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new_dev_df_bert = utils.get_df_bert_multilabel(new_dev_df)\n",
    "# new_dev_df_bert.to_csv(tsv_out_res('development/dev.tsv'), sep='\\t', index=False, header=False)\n",
    "# # don't need to keep rewriting these so can comment above out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the original train and test files (that are to be used for reporting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_df_bert = utils.get_df_bert_multilabel(train_df)\n",
    "# train_df_bert.to_csv(tsv_out_res('testing/train.tsv'), sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_df_bert = utils.get_df_bert_multilabel(test_df)\n",
    "# test_df_bert.to_csv(tsv_out_res('testing/dev.tsv'), sep='\\t', index=False, header=False)\n",
    "# # we must call it dev for the purposes of the evaluation - that is just the name that the algorithm expects\n",
    "# # this can probably be changed in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {1:'sd', 2:'sdas'}\n",
    "list(a.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
